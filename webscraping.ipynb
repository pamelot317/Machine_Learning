{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import requests_ftp\n",
    "import requests_cache\n",
    "import lxml\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from scipy.misc import imread\n",
    "plt.style.use('ggplot')\n",
    "requests_cache.install_cache('../bloomberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def motley_page_links(page):\n",
    "    \n",
    "    response = requests.get(\n",
    "        'https://www.fool.com/search/solr.aspx?page={}&q=apple&sort=date&source=isesitbut0000001'.format(page))\n",
    "    response.raise_for_status()\n",
    "    html = response.text\n",
    "    parsed_html = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "    div_with_links = parsed_html.find_all(name = 'dl',\n",
    "                                         attrs = {'class' : 'results'})\n",
    "    links = []\n",
    "    for link in div_with_links[0].find_all('a', href = True):\n",
    "        links.append(link['href'])\n",
    "    \n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def motley_all_links(no_pages = 1):\n",
    "    \"\"\"\n",
    "    Given number of pages, it returns all the links \n",
    "    from \"no_pages\"\n",
    "    \n",
    "    Input: number of pages (default = 1)\n",
    "    Output: a list with links from the pages\n",
    "    \"\"\"\n",
    "    all_links = []\n",
    "    for page in range(1, (no_pages + 1)):\n",
    "        all_links.extend(motley_page_links(page))\n",
    "    \n",
    "    return all_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def motley_article_info(url):\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    html = response.text\n",
    "    parsed_html = BeautifulSoup(html, 'lxml')\n",
    "    content = parsed_html.find_all(name = 'div',\n",
    "                                      attrs = {'class' : 'full_article'})\n",
    "\n",
    "    date = parsed_html.find_all(name = 'div', attrs = {'class' : 'publication-date'})[0].text.strip()\n",
    "    title = parsed_html.find_all('h1')[0].text\n",
    "    article = ' '.join([t.text for t in content[0].find_all('p')])\n",
    "    \n",
    "    return {'title'   : title,\n",
    "            'date'    : date,\n",
    "            'article' : article,\n",
    "            'url'     : url}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def motley_df(no_pages):\n",
    "    \"\"\"\n",
    "    Creates DataFrame for the articles in url\n",
    "    with author, text, title, and url as column\n",
    "    names.\n",
    "    \n",
    "    Input: A url, number of pages\n",
    "    Output: DataFrame with 4 columns: author,\n",
    "    text, title, and url.\n",
    "    \"\"\"\n",
    "    \n",
    "    #get all links in the specified number of pages\n",
    "    #from url\n",
    "    links = motley_all_links(no_pages)\n",
    "    \n",
    "    #create dataframe for each link and\n",
    "    #combine them into one dataframe\n",
    "    article_df = pd.DataFrame(index = [999999], columns=['article', 'date', 'title', 'url'])\n",
    "    for i, link in enumerate(links):\n",
    "        try:\n",
    "            append_to = pd.DataFrame(motley_article_info(link), index = [i])\n",
    "            article_df = article_df.append(append_to)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    article_df = article_df.drop(999999)\n",
    "    return article_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df = motley_df(28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check current directory for motleyfool.csv\n"
     ]
    }
   ],
   "source": [
    "#convert_to_csv(df, \"motleyfool.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_page_links(page):\n",
    "    \"\"\"\n",
    "    Given a page number, it returns all article links.\n",
    "    \n",
    "    Input: a page number (default = 1)\n",
    "    Output: a list with links on the given page\n",
    "    \"\"\"\n",
    "    \n",
    "    response = requests.get(\n",
    "        'https://www.bloomberg.com/search?query=apple&endTime=2017-06-02T14:25:01.383Z&page={}'.format(page))\n",
    "    response.raise_for_status()\n",
    "    html = response.text\n",
    "    parsed_html = BeautifulSoup(html, 'lxml')\n",
    "    div_with_links = parsed_html.find_all(name = 'article', \n",
    "                                          attrs = {'class' : 'search-result-story type-article'})\n",
    "    links = []\n",
    "    for tag in div_with_links:\n",
    "        try:\n",
    "            links.append(tag.find_all('a', href = True)[1]['href'])\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_all_links(no_pages = 1):\n",
    "    \"\"\"\n",
    "    Given number of pages, it returns all the links \n",
    "    from \"no_pages\"\n",
    "    \n",
    "    Input: number of pages (default = 1)\n",
    "    Output: a list with links from the pages\n",
    "    \"\"\"\n",
    "    all_links = []\n",
    "    for page in range(1, (no_pages + 1)):\n",
    "        all_links.extend(get_page_links(page))\n",
    "    \n",
    "    return all_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_article_info(url):\n",
    "    \"\"\"\n",
    "    Given an article url, it returns title, date, content\n",
    "    and url of that article.\n",
    "    \n",
    "    Input: article url\n",
    "    Ouput: a dictionary with 'title', 'date',\n",
    "    'article', and 'url' as keys.\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    html = response.text\n",
    "    parsed_html = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "    content = parsed_html.find_all(name = 'div',\n",
    "                                  attrs = {'class' : 'transporter-item current'})\n",
    "\n",
    "    content_text = content[0].find_all(name = 'div', attrs = {'class' : 'body-copy'})\n",
    "\n",
    "    article = ' '.join([t.text for t in content_text[0].find_all(name = 'p')])\n",
    "\n",
    "    title = content[0].find_all(name = 'h1', attrs = {'class' : 'lede-text-only__hed'})[0].text\n",
    "\n",
    "    date = content[0].find_all(name = 'time', attrs = {'class' : 'article-timestamp'})[0].text[-30:]\n",
    "    \n",
    "    return {'title'   : title,\n",
    "            'date'    : date,\n",
    "            'article' : article,\n",
    "            'url'     : url}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_df(no_pages):\n",
    "    \"\"\"\n",
    "    Creates DataFrame for the articles in url\n",
    "    with author, text, title, and url as column\n",
    "    names.\n",
    "    \n",
    "    Input: A url, number of pages\n",
    "    Output: DataFrame with 4 columns: author,\n",
    "    text, title, and url.\n",
    "    \"\"\"\n",
    "    \n",
    "    #get all links in the specified number of pages\n",
    "    #from url\n",
    "    links = get_all_links(no_pages)\n",
    "    \n",
    "    #create dataframe for each link and\n",
    "    #combine them into one dataframe\n",
    "    article_df = pd.DataFrame(index = [999999], columns=['article', 'date', 'title', 'url'])\n",
    "    for i, link in enumerate(links):\n",
    "        try:\n",
    "            append_to = pd.DataFrame(get_article_info(link), index = [i])\n",
    "            article_df = article_df.append(append_to)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    article_df = article_df.drop(999999)\n",
    "    return article_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_csv(df, name):\n",
    "    df.to_csv(name, index=False, encoding='utf-8')\n",
    "    print 'check current directory for {}'.format(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df = create_df(33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check current directory for \"bloomberg.csv\"\n"
     ]
    }
   ],
   "source": [
    "#convert_to_csv(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'   \\nJune 1, 2017, 8:24 PM EDT\\n' u'   \\nJune 1, 2017, 6:00 AM EDT\\n'\n",
      " u'   \\nMay 31, 2017, 3:21 PM EDT\\n' u'  \\nJune 1, 2017, 10:00 AM EDT\\n'\n",
      " u'   \\nJune 1, 2017, 6:00 PM EDT\\n' u'   \\nJune 1, 2017, 4:02 PM EDT\\n'\n",
      " u'   \\nJune 1, 2017, 5:55 AM EDT\\n' u'   \\nJune 1, 2017, 6:00 AM EDT\\n'\n",
      " u'   \\nMay 31, 2017, 6:00 AM EDT\\n' u'   \\nJune 1, 2017, 6:02 AM EDT\\n'\n",
      " u'  \\nMay 31, 2017, 12:04 PM EDT\\n' u'   \\nJune 1, 2017, 7:44 PM EDT\\n'\n",
      " u'   \\nMay 31, 2017, 6:00 AM EDT\\n' u'   \\nJune 1, 2017, 4:41 PM EDT\\n'\n",
      " u'   \\nMay 31, 2017, 4:20 PM EDT\\n' u'   \\nJune 1, 2017, 4:14 AM EDT\\n'\n",
      " u'   \\nJune 2, 2017, 9:27 AM EDT\\n' u'   \\nJune 1, 2017, 2:32 PM EDT\\n'\n",
      " u'   \\nJune 1, 2017, 3:50 PM EDT\\n' u'   \\nMay 31, 2017, 7:47 AM EDT\\n'\n",
      " u'  \\nJune 1, 2017, 12:01 AM EDT\\n' u'   \\nJune 1, 2017, 4:30 PM EDT\\n'\n",
      " u'   \\nJune 1, 2017, 7:12 AM EDT\\n' u'  \\nMay 30, 2017, 12:13 PM EDT\\n'\n",
      " u'   \\nMay 30, 2017, 8:24 PM EDT\\n' u'   \\nMay 31, 2017, 9:02 AM EDT\\n'\n",
      " u'   \\nMay 31, 2017, 1:58 PM EDT\\n' u'   \\nMay 31, 2017, 8:39 AM EDT\\n'\n",
      " u'   \\nMay 30, 2017, 6:49 PM EDT\\n' u'  \\nMay 30, 2017, 12:16 PM EDT\\n'\n",
      " u'  \\nMay 30, 2017, 11:08 PM EDT\\n' u'   \\nMay 31, 2017, 9:30 AM EDT\\n'\n",
      " u'   \\nMay 30, 2017, 4:30 PM EDT\\n' u'   \\nMay 24, 2017, 8:33 AM EDT\\n'\n",
      " u'   \\nMay 25, 2017, 2:00 AM EDT\\n' u'  \\nMay 24, 2017, 12:28 PM EDT\\n'\n",
      " u'   \\nMay 22, 2017, 7:23 PM EDT\\n' u'   \\nMay 22, 2017, 6:53 PM EDT\\n'\n",
      " u'   \\nMay 25, 2017, 4:15 PM EDT\\n' u'   \\nMay 25, 2017, 2:41 PM EDT\\n'\n",
      " u'   \\nMay 24, 2017, 6:31 AM EDT\\n' u'   \\nMay 24, 2017, 6:31 AM EDT\\n'\n",
      " u'   \\nMay 24, 2017, 1:00 PM EDT\\n' u'   \\nMay 23, 2017, 7:00 AM EDT\\n'\n",
      " u'   \\nMay 24, 2017, 5:00 AM EDT\\n' u'   \\nMay 23, 2017, 8:15 AM EDT\\n'\n",
      " u'  \\nMay 23, 2017, 12:27 PM EDT\\n' u'   \\nMay 24, 2017, 5:00 AM EDT\\n'\n",
      " u'   \\nMay 23, 2017, 3:35 AM EDT\\n' u'   \\nMay 23, 2017, 5:00 PM EDT\\n'\n",
      " u'   \\nMay 22, 2017, 9:36 AM EDT\\n' u'   \\nMay 17, 2017, 2:03 PM EDT\\n'\n",
      " u'  \\nMay 17, 2017, 12:37 PM EDT\\n' u'   \\nMay 22, 2017, 5:00 AM EDT\\n'\n",
      " u'  \\nMay 21, 2017, 12:01 PM EDT\\n' u'  \\nMay 20, 2017, 10:19 AM EDT\\n'\n",
      " u'  \\nMay 19, 2017, 11:07 AM EDT\\n' u'   \\nMay 19, 2017, 8:25 AM EDT\\n'\n",
      " u'   \\nMay 17, 2017, 2:39 PM EDT\\n' u'  \\nMay 20, 2017, 11:37 AM EDT\\n'\n",
      " u'   \\nMay 20, 2017, 1:41 AM EDT\\n' u'   \\nMay 19, 2017, 7:00 AM EDT\\n'\n",
      " u'   \\nMay 19, 2017, 1:01 AM EDT\\n' u'   \\nMay 18, 2017, 4:00 PM EDT\\n'\n",
      " u'   \\nMay 19, 2017, 7:05 PM EDT\\n' u'  \\nMay 18, 2017, 12:28 PM EDT\\n'\n",
      " u'   \\nMay 18, 2017, 6:04 PM EDT\\n' u'  \\nMay 18, 2017, 11:38 PM EDT\\n'\n",
      " u'   \\nMay 18, 2017, 6:00 PM EDT\\n' u'   \\nMay 17, 2017, 6:00 AM EDT\\n'\n",
      " u'   \\nMay 17, 2017, 5:52 PM EDT\\n' u'   \\nMay 19, 2017, 9:32 AM EDT\\n'\n",
      " u'   \\nMay 18, 2017, 1:49 PM EDT\\n' u'   \\nMay 17, 2017, 5:00 AM EDT\\n'\n",
      " u'   \\nMay 17, 2017, 6:27 AM EDT\\n' u'   \\nMay 16, 2017, 6:30 AM EDT\\n'\n",
      " u'   \\nMay 17, 2017, 6:00 AM EDT\\n' u'   \\nMay 17, 2017, 2:21 PM EDT\\n'\n",
      " u'  \\nMay 18, 2017, 12:01 AM EDT\\n' u'   \\nMay 17, 2017, 4:14 PM EDT\\n'\n",
      " u'   \\nMay 17, 2017, 5:05 AM EDT\\n' u'   \\nMay 18, 2017, 4:30 AM EDT\\n'\n",
      " u'   \\nMay 17, 2017, 4:51 PM EDT\\n' u'  \\nMay 17, 2017, 11:31 AM EDT\\n'\n",
      " u'   \\nMay 16, 2017, 4:22 PM EDT\\n' u'   \\nMay 18, 2017, 3:30 AM EDT\\n'\n",
      " u'   \\nMay 17, 2017, 9:00 AM EDT\\n' u'   \\nMay 16, 2017, 7:38 AM EDT\\n'\n",
      " u'  \\nMay 15, 2017, 10:07 AM EDT\\n' u'   \\nMay 15, 2017, 9:00 AM EDT\\n'\n",
      " u'  \\nMay 16, 2017, 12:44 PM EDT\\n' u'   \\nMay 15, 2017, 4:44 PM EDT\\n'\n",
      " u'   \\nMay 16, 2017, 5:00 AM EDT\\n' u'  \\nMay 10, 2017, 11:23 PM EDT\\n'\n",
      " u'  \\nMay 15, 2017, 12:54 PM EDT\\n' u'   \\nMay 10, 2017, 9:47 PM EDT\\n'\n",
      " u'  \\nMay 15, 2017, 12:10 PM EDT\\n' u'   \\nMay 15, 2017, 8:09 PM EDT\\n'\n",
      " u'   \\nMay 15, 2017, 6:00 AM EDT\\n' u'  \\nMay 14, 2017, 11:35 PM EDT\\n'\n",
      " u'   \\nMay 11, 2017, 6:00 AM EDT\\n' u'   \\nMay 15, 2017, 6:25 AM EDT\\n'\n",
      " u'    \\nMay 9, 2017, 4:47 PM EDT\\n' u'   \\nMay 14, 2017, 9:56 PM EDT\\n'\n",
      " u'  \\nMay 15, 2017, 12:00 PM EDT\\n' u'    \\nMay 9, 2017, 7:30 PM EDT\\n'\n",
      " u'    \\nMay 9, 2017, 5:09 PM EDT\\n' u'  \\nMay 12, 2017, 10:47 AM EDT\\n'\n",
      " u'    \\nMay 8, 2017, 5:47 PM EDT\\n' u'   \\nMay 9, 2017, 10:00 AM EDT\\n'\n",
      " u'   \\nMay 10, 2017, 3:23 AM EDT\\n' u'    \\nMay 8, 2017, 6:30 AM EDT\\n'\n",
      " u'   \\nMay 12, 2017, 6:00 AM EDT\\n' u'  \\nMay 12, 2017, 10:45 AM EDT\\n'\n",
      " u'   \\nMay 11, 2017, 1:47 AM EDT\\n' u'   \\nMay 12, 2017, 1:43 PM EDT\\n'\n",
      " u'   \\nMay 11, 2017, 8:16 PM EDT\\n' u'  \\nMay 11, 2017, 11:30 AM EDT\\n'\n",
      " u'   \\nMay 12, 2017, 9:12 AM EDT\\n' u'   \\nMay 11, 2017, 6:46 PM EDT\\n'\n",
      " u'   \\nMay 11, 2017, 5:00 AM EDT\\n' u'   \\nMay 11, 2017, 5:00 AM EDT\\n'\n",
      " u'   \\nMay 11, 2017, 1:37 AM EDT\\n' u'  \\nMay 10, 2017, 10:40 PM EDT\\n'\n",
      " u'  \\nMay 10, 2017, 11:37 PM EDT\\n' u'    \\nMay 9, 2017, 6:00 AM EDT\\n'\n",
      " u'   \\nMay 10, 2017, 5:00 PM EDT\\n' u'  \\nMay 10, 2017, 11:38 AM EDT\\n'\n",
      " u'   \\nMay 9, 2017, 12:13 PM EDT\\n' u'    \\nMay 9, 2017, 6:50 AM EDT\\n'\n",
      " u'    \\nMay 5, 2017, 1:14 AM EDT\\n' u'   \\nMay 10, 2017, 2:30 AM EDT\\n'\n",
      " u'    \\nMay 4, 2017, 6:00 AM EDT\\n' u'    \\nMay 9, 2017, 8:33 AM EDT\\n'\n",
      " u'    \\nMay 8, 2017, 6:09 PM EDT\\n' u'    \\nMay 9, 2017, 5:01 AM EDT\\n'\n",
      " u'    \\nMay 8, 2017, 6:00 AM EDT\\n' u'    \\nMay 8, 2017, 5:07 PM EDT\\n'\n",
      " u'   \\nMay 8, 2017, 11:25 PM EDT\\n' u'    \\nMay 8, 2017, 7:12 PM EDT\\n'\n",
      " u'    \\nMay 7, 2017, 6:55 PM EDT\\n' u'    \\nMay 3, 2017, 8:55 AM EDT\\n'\n",
      " u'    \\nMay 8, 2017, 6:26 PM EDT\\n' u'    \\nMay 8, 2017, 5:11 AM EDT\\n'\n",
      " u'    \\nMay 9, 2017, 1:00 AM EDT\\n' u'   \\nMay 7, 2017, 11:51 PM EDT\\n'\n",
      " u'    \\nMay 3, 2017, 6:33 PM EDT\\n' u'    \\nMay 9, 2017, 9:28 AM EDT\\n'\n",
      " u'    \\nMay 7, 2017, 5:00 PM EDT\\n' u'   \\nMay 3, 2017, 12:58 PM EDT\\n'\n",
      " u'   \\nMay 6, 2017, 12:30 PM EDT\\n' u'    \\nMay 2, 2017, 4:39 PM EDT\\n'\n",
      " u'    \\nMay 2, 2017, 4:38 PM EDT\\n' u'    \\nMay 2, 2017, 6:11 PM EDT\\n'\n",
      " u'    \\nMay 3, 2017, 3:56 PM EDT\\n' u'    \\nMay 3, 2017, 5:57 AM EDT\\n'\n",
      " u'    \\nMay 4, 2017, 8:05 AM EDT\\n' u'    \\nMay 5, 2017, 4:51 PM EDT\\n'\n",
      " u'   \\nMay 2, 2017, 10:50 AM EDT\\n' u'   \\nMay 2, 2017, 11:52 AM EDT\\n'\n",
      " u'    \\nMay 5, 2017, 5:36 AM EDT\\n' u'    \\nMay 3, 2017, 4:55 PM EDT\\n'\n",
      " u'    \\nMay 4, 2017, 9:00 AM EDT\\n' u'    \\nMay 3, 2017, 8:04 AM EDT\\n'\n",
      " u'    \\nMay 4, 2017, 4:02 PM EDT\\n' u'    \\nMay 5, 2017, 2:00 AM EDT\\n'\n",
      " u'    \\nMay 3, 2017, 3:00 AM EDT\\n' u'    \\nMay 3, 2017, 6:00 AM EDT\\n'\n",
      " u'    \\nMay 4, 2017, 8:30 AM EDT\\n' u'   \\nMay 3, 2017, 11:14 PM EDT\\n'\n",
      " u'    \\nMay 4, 2017, 5:00 AM EDT\\n' u'    \\nMay 3, 2017, 3:28 AM EDT\\n'\n",
      " u'    \\nMay 4, 2017, 7:00 AM EDT\\n' u'    \\nMay 3, 2017, 6:00 AM EDT\\n'\n",
      " u'   \\nMay 1, 2017, 12:01 AM EDT\\n' u'    \\nMay 4, 2017, 4:00 AM EDT\\n'\n",
      " u'    \\nMay 3, 2017, 4:25 PM EDT\\n' u'    \\nMay 3, 2017, 8:00 AM EDT\\n'\n",
      " u'    \\nMay 2, 2017, 4:02 PM EDT\\n' u'    \\nMay 2, 2017, 7:35 AM EDT\\n'\n",
      " u'   \\nMay 3, 2017, 12:20 PM EDT\\n' u' \\nApril 28, 2017, 7:32 AM EDT\\n'\n",
      " u'    \\nMay 2, 2017, 5:51 AM EDT\\n' u'   \\nMay 2, 2017, 10:31 PM EDT\\n'\n",
      " u' \\nApril 28, 2017, 1:40 PM EDT\\n' u'    \\nMay 2, 2017, 8:02 PM EDT\\n'\n",
      " u'   \\nMay 2, 2017, 12:59 PM EDT\\n' u' \\nApril 27, 2017, 5:49 PM EDT\\n'\n",
      " u'   \\nMay 2, 2017, 12:22 PM EDT\\n' u'    \\nMay 2, 2017, 6:12 AM EDT\\n'\n",
      " u' \\nApril 29, 2017, 3:04 AM EDT\\n' u'    \\nMay 1, 2017, 6:12 PM EDT\\n'\n",
      " u'    \\nMay 1, 2017, 9:16 AM EDT\\n' u' \\nApril 27, 2017, 5:00 AM EDT\\n'\n",
      " u'    \\nMay 1, 2017, 4:30 PM EDT\\n' u' \\nApril 26, 2017, 6:00 AM EDT\\n']\n"
     ]
    }
   ],
   "source": [
    "print df['date'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
